{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 20:19:30.958821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-14 20:19:30.969075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736857170.984791   12456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736857170.990274   12456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-14 20:19:31.006246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# 這兩個套件是為了要將訓練資料的路徑讀取工具也引進來\n",
    "\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf # 就是 Tensorflow 不用多說\n",
    "import tensorflow_hub as hub # Tensorflow bert 社群\n",
    "import tensorflow_text as text # 前處理\n",
    "from official.nlp import optimization  # 優化工具\n",
    "\n",
    "import matplotlib.pyplot as plt # 畫圖用的套件\n",
    "\n",
    "tf.get_logger().setLevel('ERROR') # Debug的工具，也可以不寫.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736857173.147195   12456 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1401 MB memory:  -> device: 0, name: NVIDIA GeForce MX550, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "    \n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8888880915860543667\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1469382656\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9903331169386631404\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce MX550, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736857173.220872   12456 gpu_device.cc:2022] Created device /device:GPU:0 with 1401 MB memory:  -> device: 0, name: NVIDIA GeForce MX550, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0000 00:00:1736857173.228561   12456 gpu_device.cc:2022] Created device /device:GPU:0 with 1401 MB memory:  -> device: 0, name: NVIDIA GeForce MX550, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "assert tf.test.is_gpu_available()\n",
    "assert tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96mINFO\u001b[0m  2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736857173.241678   12456 gpu_device.cc:2022] Created device /device:GPU:0 with 1401 MB memory:  -> device: 0, name: NVIDIA GeForce MX550, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.general import *\n",
    "\n",
    "info(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 這個是資料集的下載連結，如果你複製這段 url 到瀏覽器上，也可以下載的到\n",
    "# url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "# dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "#                                   untar=True, cache_dir='.',\n",
    "#                                   cache_subdir='')\n",
    "\n",
    "# # 所謂的 os.path.join 就是把兩個路徑結合起來的意思，如果路徑是 ./data/，join之後就會變成 ./data/aclImdb，而以下所做的，就是要整合資料路徑，方便之後模型讀取\n",
    "# dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "# train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# # remove unused folders to make it easier to load the data\n",
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>isDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch new seri squid game mashtali soroush dec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>today math lesson teacher told vovochka euler ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sequenc integ call good max min sequenc integ ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petya got interest grammar third year school i...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nezzar design brand new game hidden permut sha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>given array integ find maximum possibl valu am...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>polycarp poor memori day rememb 3 differ lette...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>hard version problem differ easi version versi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>two chess piec rook knight stand standard ches...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>first line contain singl integ 1 32 number tes...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7976 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Statement   isDP\n",
       "0     watch new seri squid game mashtali soroush dec...  False\n",
       "1     today math lesson teacher told vovochka euler ...  False\n",
       "2     sequenc integ call good max min sequenc integ ...   True\n",
       "3     petya got interest grammar third year school i...  False\n",
       "4     nezzar design brand new game hidden permut sha...  False\n",
       "...                                                 ...    ...\n",
       "7971  given array integ find maximum possibl valu am...   True\n",
       "7972  polycarp poor memori day rememb 3 differ lette...  False\n",
       "7973  hard version problem differ easi version versi...  False\n",
       "7974  two chess piec rook knight stand standard ches...  False\n",
       "7975  first line contain singl integ 1 32 number tes...  False\n",
       "\n",
       "[7976 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.general import *\n",
    "ls('../data')\n",
    "\n",
    "import pandas as pd\n",
    "f = pd.read_csv('../data/filtered_train_data.csv')\n",
    "f = f[['Statement', 'tags']]\n",
    "f['isDP'] = ['dp' in i for i in f['tags'].tolist()]\n",
    "f = f.drop('tags', axis = 1)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = f.to_dict()\n",
    "# for i in train:\n",
    "# debug(pwd())\n",
    "cd(root)\n",
    "cd('aclImdb/train')\n",
    "for tag in ['pos', True], ['neg', False]:\n",
    "    cd(tag[0])\n",
    "    for i in range(len(f['Statement'])):\n",
    "        if pre['isDP'][i] ^ tag[1]: continue;\n",
    "        write_to_file(f'{i}.txt', pre['Statement'][i])\n",
    "    cd('..')\n",
    "cd('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96mINFO\u001b[0m  Loading Training Data.\n",
      "Found 7976 files belonging to 2 classes.\n",
      "Using 6381 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1736857173.849509   12652 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/ysh/miniconda3/envs/gpu-old/lib/python3.9/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/ysh/miniconda3/envs/gpu-old/lib/python3.9/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/ysh/miniconda3/envs/gpu-old/lib/python3.9/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96mINFO\u001b[0m  Loading Validating Data.\n",
      "Found 7976 files belonging to 2 classes.\n",
      "Using 1595 files for validation.\n",
      "\u001b[1m\u001b[96mINFO\u001b[0m  Loading Testing Data.\n",
      "Found 1995 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "info('Loading Training Data.')\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "info('Loading Validating Data.')\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "info('Loading Testing Data.')\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'interact n point cartesian plane denot point coordin integ inclus three point collinear say point vertic convex polygon word exist permut integ 1 n polygon convex vertic list give number n hide coordin point task guess permut ask multipl queri queri give khanh 4 integ j k either 1 2 j k three distinct indic 1 n inclus respons khanh tell 1 area triangl multipli 2 2 sign cross product two vector recal cross product vector vector b integ sign number 1 posit otherwis proven cross product obtain queri 0 ask 3 n note khanh fix coordin point chang answer queri need guess coordin permut equal 1 indic vertic list order'\n",
      "Label : 0 (neg)\n",
      "Label : 0 (neg)\n",
      "Label : 0 (neg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 20:19:34.657999: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-01-14 20:19:34.659029: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    print(f'Review: {text_batch.numpy()[0]}')\n",
    "    for i in range(3):\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\" # 這是預訓練模型\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\" # 這是 BERT 前處理需要用的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mDEBUG\u001b[0m watch new seri squid game mashtali soroush decid hold squid game soroush agre host provid money winner prize mashtali becam front man player regist play game win great prize mashtali found huge winner prize go decid kill elimin player could take money evil mashtali go elimin player unroot tree n vertic everi player 2 special vertic one oper mashtali choos vertex v tree remain player find vertex w simpl path closest v player mashtali wonder minimum number oper perform remov everi player game take money sinc think money could solv problem ask help\n"
     ]
    }
   ],
   "source": [
    "debug(pre['Statement'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [  101  3422  2047 14262  2072 26852  2208 16137 22893  3669  2061 13288]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) # 呼叫前處理模型\n",
    "text_test = [pre['Statement'][0]] # 先輸入簡單的句子看看會變成什麼樣子\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "debug('---')\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.save_weights('model.st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug([pre['isDP'][i] for i in list(pre['isDP'])].count(True))\n",
    "debug(len(pre['isDP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/filtered_test_data.csv')\n",
    "df = df[['Statement', 'tags']]\n",
    "df['isDP'] = ['dp' in i for i in df['tags'].tolist()]\n",
    "df = df.drop('tags', axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = df.to_dict()\n",
    "n = len(now['Statement'])\n",
    "cd(root)\n",
    "cd('aclImdb')\n",
    "cd ('test')\n",
    "for tag in ['neg', 0], ['pos', 1]:\n",
    "    cd(tag[0])\n",
    "    for i in range(n):\n",
    "        if now['isDP'][i] ^ tag[1]: continue;\n",
    "        info(f'[{i} / {n}] Parsing Rows', end = '\\r')\n",
    "        write_to_file(f'{i}.txt', now['Statement'][i])\n",
    "    info(f'[OK] Task Parse {tag[0]} Finished.')\n",
    "    cd('..')\n",
    "cd('../..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
